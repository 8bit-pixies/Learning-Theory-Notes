Link: http://ttic.uchicago.edu/~tewari/LT_SP2008.html

|Lecture     | Topics Covered                                                               |
|------------|------------------------------------------------------------------------------|
|Lecture  1  | Mistake bound model, Halving algorithm, Linear classifiers and margin        |
|Lecture  2  | Perceptron algorithm, Lower bound for L2-margin, Winnow                      |
|Lecture  3  | Winnow (contd.), Online Convex Programming, Online Gradient Descent          |
|Lecture  4  | Exponentiated Gradient Descent, Applications of Online Convex Programming    |
|Lecture  5  | Proof of von Neumann's Minmax Theorem, Weak and Strong Learning, Boosting    |
|Lecture  6  | AdaBoost, L1 Margins and Weak Learning                                       |
|Lecture  7  | Probabilistic Setup, Loss functions, Empirical Risk Minimization (ERM)       |
|Lecture  8  | Concentration, ERM, Compression Bounds                                       |
|Lecture  9  | Compression Bounds (contd.), Rademacher averages                             |
|Lecture 10  | Massart's Finite Class Lemma, Growth Function                                |
|Lecture 11  | VC Dimension, Sauer's Lemma                                                  |
|Lecture 12  | VC Dimension of Multi-layer Neural Networks, Range Queries                   |
|Lecture 13  | Online to Batch Conversions                                                  |
|Lecture 13a | (Exponentiated) Stochastic Gradient Descent for L1 Constrained Problems      |
|Lecture 14  | Covering Numbers and Rademacher Averages                                     |
|Lecture 15  | Dudley's Theorem, Pseudodimension, Fat Shattering Dimension, Packing Numbers |
|Lecture 16  | Fat Shattering Dimension and Covering Numbers                                |
|Lecture 17  | Rademacher Composition and Linear Prediction                                 |
